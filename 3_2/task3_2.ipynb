{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User 6\n",
      "814  ('Great Day in Harlem, A (1994)', '01-Jan-1994')                       5\n",
      "1536 ('Aiqing wansui (1994)', '22-Jul-1996')                                5\n",
      "1512 ('World of Apu, The (Apur Sansar) (1959)', '05-Apr-1996')              4.948\n",
      "1599 (\"Someone Else's America (1995)\", '10-May-1996')                       4.884\n",
      "1500 ('Santa with Muscles (1996)', '08-Nov-1996')                           4.882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Great Day in Harlem, A ',\n",
       " 'Aiqing wansui ',\n",
       " 'World of Apu, The (Apur Sansar) ',\n",
       " \"Someone Else's America \",\n",
       " 'Santa with Muscles ']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import get_dataset_dir\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def read_item_names():\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = (line[1], line[2])\n",
    "    return rid_to_name\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=5):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, round(est, 3)))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "user_id = \"6\"\n",
    "\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'cosine', 'user_based': True, 'min_support': 5}\n",
    "algo = KNNBaseline(k=4, sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "\n",
    "testset = trainset.build_anti_testset()\n",
    "testset = filter(lambda x: x[0] == user_id, testset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions)\n",
    "rid_to_name = read_item_names()\n",
    "\n",
    "print('User ' + user_id)\n",
    "films_list = []\n",
    "for movie_rid, rating in top_n[user_id]:\n",
    "    films_list.append(rid_to_name[movie_rid][0][:-6])\n",
    "    print('{:4s} {:70s} {}'.format(movie_rid, str(rid_to_name[movie_rid]), rating))\n",
    "    \n",
    "films_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Great Day in Harlem',\n",
       " 'Aiqing wansui',\n",
       " 'Apur Sansar',\n",
       " \"Someone Else's America\",\n",
       " 'Santa with Muscles']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#правильное написание фильмов для поиска в wikidata\n",
    "for i in range(len(films_list)):\n",
    "    if (',' in films_list[i]):\n",
    "        temp=films_list[i].split(',')\n",
    "        films_list[i]=temp[1]+temp[0]\n",
    "    if (re.search(r'\\((.*?)\\)',films_list[i])):\n",
    "        films_list[i] = re.search(r'\\((.*?)\\)',films_list[i])\n",
    "        films_list[i] = (films_list[i].group(0)).replace('(', '').replace(')', '')\n",
    "    films_list[i]=films_list[i].strip()\n",
    "\n",
    "films_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"https://www.wikidata.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q4657171', 'A Great Day in Harlem'),\n",
       " ('Q622376', 'Apur Sansar'),\n",
       " ('Q7219297', \"Someone Else's America\"),\n",
       " ('Q1631700', 'Santa with Muscles')]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_films=[]\n",
    "\n",
    "for query in films_list:\n",
    "    params = {\n",
    "        'action' : 'wbsearchentities',\n",
    "        'format' : 'json',\n",
    "        'language' : 'en',\n",
    "        'search': query\n",
    "    }\n",
    "    res = requests.get(API_ENDPOINT, params = params)\n",
    "    for film in res.json()['search']:\n",
    "        if ('description' in film):\n",
    "            if ('film' in film['description']):\n",
    "                query_films.append((film['id'], query))\n",
    "                \n",
    "query_films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Great Day in Harlem  : nothing similar found\n",
      "Apur Sansar  : nothing similar found\n",
      "Someone Else's America  : nothing similar found\n",
      "Santa with Muscles  : nothing similar found\n"
     ]
    }
   ],
   "source": [
    "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "\n",
    "for film in query_films:\n",
    "    spaqrql_query = \"\"\"\n",
    "    SELECT DISTINCT ?film ?filmLabel \n",
    "    WHERE \n",
    "    {\n",
    "      ?film wdt:P31 wd:Q11424.\n",
    "      wd:\"\"\"+film[0]+\"\"\" wdt:P136 ?genre.\n",
    "      wd:\"\"\"+film[0]+\"\"\" wdt:P272 ?production_company.\n",
    "      ?film wdt:P136 ?genre1.\n",
    "      ?film wdt:P272 ?production_company.\n",
    "      FILTER(?film != wd:\"\"\"+film[0]+\"\"\").\n",
    "      FILTER(?genre1 NOT IN (?genre)).\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "    }\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "\n",
    "    sparql.setQuery(spaqrql_query)\n",
    "\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    results_df = pd.io.json.json_normalize(results['results']['bindings'])\n",
    "    if len(results_df.columns) <= 0:\n",
    "        print(film[1], \" : nothing similar found\")\n",
    "    else:\n",
    "        print(film[1])\n",
    "        print(results_df[['film.value', 'filmLabel.value']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}